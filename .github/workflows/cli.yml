name: CLI weekly live test

on:
  schedule:
    - cron: "0 18 * * 0"
  workflow_dispatch:
    inputs:
      test-cases:
        description: 'Specific test cases to run. Example: ["test_integration_create_aws", "test_integration_delete"]. Leave empty to run all test cases'
        required: false
        type: string

permissions:
  id-token: write
  contents: read

jobs:
  setup:
    runs-on: ubuntu-latest
    environment: engineering
    outputs:
      test-cases: ${{ steps.discover-tests.outputs.test-cases }}
      python-versions: ${{ steps.discover-tests.outputs.python-versions }}
    steps:
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11"

      - name: Azure login
        uses: azure/login@v2
        with:
          client-id: ${{secrets.APICEXT_TEST_AZURE_CLIENT_ID}}
          tenant-id: ${{secrets.APICEXT_TEST_AZURE_TENANT_ID}}
          subscription-id: ${{secrets.APICEXT_TEST_AZURE_SUBSCRIPTION_ID}}
          enable-AzPSSession: true

      - name: Clone azure-cli-extensions repository
        run: |
          git clone https://github.com/blackchoey/azure-cli-extensions.git
          cd azure-cli-extensions
          git checkout main

      - name: Setup development environment
        run: |
          cd azure-cli-extensions
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install -U pip
          pip install azdev pytest
          azdev setup -r .
          azdev extension add apic-extension

      - name: Discover test cases and set outputs
        id: discover-tests
        run: |
          cd azure-cli-extensions
          source .venv/bin/activate
          cd src/apic-extension

          # Check if specific test cases are provided
          test_input='${{ inputs.test-cases }}'
          if [ -n "$test_input" ] && [ "$test_input" != "null" ] && [ "$test_input" != "" ]; then
            echo "Using specific test cases from input: $test_input"
            test_cases="$test_input"
          else
            echo "Discovering all test cases using pytest..."

            # Find test files and collect test cases
            test_files=$(find . -name "test_*.py" -type f | head -10)
            echo "Debug: Found test files:"
            echo "$test_files"

            if [ -z "$test_files" ]; then
              echo "❌ No test files found"
              test_cases='["full-suite"]'
            else
              # Use pytest to collect all test cases
              echo "Debug: Collecting test cases with pytest..."

              # Collect test node IDs and extract just the test function names
              test_collection=$(python -m pytest --collect-only -q 2>/dev/null | \
                grep "::test_" | \
                sed 's/.*::\(test_[^[]*\).*/\1/' | \
                sort | uniq || true)

              echo "Debug: Raw test collection output:"
              echo "$test_collection"

              if [ -z "$test_collection" ]; then
                echo "❌ No test cases found with pytest, trying alternative approach..."

                # Fallback: Parse test files directly for test function definitions
                test_collection=$(grep -h "^def test_" $(find . -name "test_*.py") | \
                  sed 's/def \(test_[^(]*\).*/\1/' | \
                  sort | uniq || true)

                echo "Debug: Fallback test collection:"
                echo "$test_collection"
              fi

              if [ -z "$test_collection" ]; then
                echo "❌ No individual test cases found, using full-suite approach"
                test_cases='["full-suite"]'
              else
                test_count=$(echo "$test_collection" | wc -l)
                echo "✅ Successfully discovered $test_count test cases"

                # Convert to JSON array
                test_cases=$(echo "$test_collection" | jq -R -s -c 'split("\n") | map(select(length > 0))')
                total_tests=$(echo "$test_cases" | jq '. | length')
                echo "Found $total_tests test cases total"

                # Validate JSON format
                if ! echo "$test_cases" | jq empty 2>/dev/null; then
                  echo "❌ Invalid JSON format, falling back to full-suite"
                  test_cases='["full-suite"]'
                fi
              fi
            fi
          fi

          echo "Final test cases: $test_cases"
          echo "test-cases=$test_cases" >> $GITHUB_OUTPUT

          # Python versions to test against
          python_versions='["3.9", "3.10", "3.11", "3.12"]'
          echo "python-versions=$python_versions" >> $GITHUB_OUTPUT
  # Execute tests using matrix strategy for all Python versions
  # This replaces the previous 4 duplicate jobs (test-python-39, test-python-310, etc.)
  # Matrix strategy automatically creates jobs: test (3.9), test (3.10), test (3.11), test (3.12)
  test:
    needs: setup
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]
    uses: ./.github/workflows/test-execution-all.yml
    with:
      test-cases: ${{ needs.setup.outputs.test-cases }}
      python-version: ${{ matrix.python-version }}
    secrets:
      APICEXT_TEST_AZURE_CLIENT_ID: ${{ secrets.APICEXT_TEST_AZURE_CLIENT_ID }}
      APICEXT_TEST_AZURE_TENANT_ID: ${{ secrets.APICEXT_TEST_AZURE_TENANT_ID }}
      APICEXT_TEST_AZURE_SUBSCRIPTION_ID: ${{ secrets.APICEXT_TEST_AZURE_SUBSCRIPTION_ID }}
      USERASSIGNED_IDENTITY: ${{ secrets.USERASSIGNED_IDENTITY }}
      AWS_ACCESS_KEY_LINK: ${{ secrets.AWS_ACCESS_KEY_LINK }}
      AWS_SECRET_ACCESS_KEY_LINK: ${{ secrets.AWS_SECRET_ACCESS_KEY_LINK }}

  # Send notification with inline logic (no reusable workflow)
  notify:
    runs-on: ubuntu-latest
    environment: engineering
    needs: [setup, test]
    if: (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch') && !cancelled()
    steps:
      - name: Generate Email Content
        id: generate-email-content
        run: |
          # Get the test cases that were executed
          test_cases='${{ needs.setup.outputs.test-cases }}'
          python_versions_input='${{ needs.setup.outputs.python-versions }}'

          echo "Analyzing test results for notification..."
          echo "Test cases: $test_cases"
          echo "Python versions: $python_versions_input"
          # Initialize temporary files for collecting results - ensure clean start
          rm -rf /tmp/test_results /tmp/current_test_cases.txt 2>/dev/null || true
          mkdir -p /tmp/test_results
          > /tmp/test_results/passed.txt
          > /tmp/test_results/failed.txt

          # Parse Python versions from JSON
          mapfile -t python_versions < <(echo "$python_versions_input" | jq -r '.[]')

          echo "Processing results for Python versions: ${python_versions[@]}"
          # Fetch all jobs from the workflow run
          jobs_response=$(curl -s -H "Accept: application/vnd.github.v3+json" \
            -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
            "https://api.github.com/repos/${{ github.repository }}/actions/runs/${{ github.run_id }}/jobs")
          echo "DEBUG: All jobs in this workflow run:"
          echo "$jobs_response" | jq -r '.jobs[] | "ID: \(.id), Name: \(.name), Conclusion: \(.conclusion), Status: \(.status)"'

          # PRIMARY APPROACH: Get results directly from the test matrix jobs
          # The main workflow has matrix jobs named like "test (3.9)", "test (3.10)", etc.
          # Each of these calls the reusable workflow which runs all test cases for that Python version
          echo "=== PRIMARY APPROACH: Analyzing test matrix job results ==="

          # Get all test matrix jobs (should be named like "test (3.9)", "test (3.10)", etc.)
          test_matrix_jobs=$(echo "$jobs_response" | jq -r '.jobs[] | select(.name | startswith("test (")) | {name: .name, conclusion: .conclusion, id: .id}')

          matrix_jobs_found=false
          if [ -n "$test_matrix_jobs" ] && [ "$test_matrix_jobs" != "" ]; then
            echo "Found test matrix jobs:"
            echo "$test_matrix_jobs" | jq -c '.'

            # Count how many valid matrix jobs we found
            matrix_job_count=$(echo "$test_matrix_jobs" | jq -s 'length')
            echo "Found $matrix_job_count matrix jobs"

            if [ "$matrix_job_count" -gt 0 ]; then
              matrix_jobs_found=true
              echo "Using matrix job results (primary approach)"


              # Process each matrix job individually to avoid subshell issues
              for python_version in "${python_versions[@]}"; do
                echo "Looking for jobs for Python version: $python_version"
                # The actual job names include test case details: "test (3.9) / test_case_name (Python 3.9)"
                # We need to find jobs that match the pattern "test (X.Y) / ..." for this Python version

                # Find all jobs for this Python version - they may have different test case names
                # Use a more robust pattern that handles parentheses correctly
                test_prefix="test ($python_version)"
                python_suffix="Python $python_version"

                echo "  Looking for jobs starting with: '$test_prefix' and containing: '$python_suffix'"

                version_jobs=$(echo "$jobs_response" | jq -r --arg prefix "$test_prefix" --arg suffix "$python_suffix" \
                  '.jobs[] | select(.name | (startswith($prefix) and contains($suffix))) | {name: .name, conclusion: .conclusion, id: .id}')

                if [ -n "$version_jobs" ] && [ "$version_jobs" != "" ] && [ "$version_jobs" != "null" ]; then
                  echo "Found jobs for Python $python_version:"
                  echo "$version_jobs" | jq -c '.'

                  # Check if ALL jobs for this Python version succeeded
                  all_success=true
                  job_count=0

                  # Count total jobs and check conclusions
                  while IFS= read -r job_line; do
                    if [ -n "$job_line" ] && [ "$job_line" != "null" ]; then
                      job_count=$((job_count + 1))
                      job_conclusion=$(echo "$job_line" | jq -r '.conclusion')
                      echo "  Job conclusion: $job_conclusion for job: $(echo "$job_line" | jq -r '.name')"
                      if [ "$job_conclusion" != "success" ]; then
                        all_success=false
                      fi
                    fi
                  done < <(echo "$version_jobs" | jq -c '.')

                  echo "  Found $job_count jobs for Python $python_version, all_success=$all_success"

                  # Count test cases for this Python version based on overall conclusion
                  test_case_count=$(echo "$test_cases" | jq '. | length')
                  echo "  Will process $test_case_count test cases for Python $python_version"

                  if [ "$all_success" = true ]; then
                    # All test cases for this Python version passed - use temp file to avoid subshell
                    echo "$test_cases" | jq -r '.[]' > "/tmp/current_test_cases.txt"
                    while IFS= read -r test_case; do
                      if [ -n "$test_case" ]; then
                        echo "$test_case (Python $python_version)" >> /tmp/test_results/passed.txt
                        echo "    ✅ Added to passed: $test_case (Python $python_version)"
                      fi
                    done < "/tmp/current_test_cases.txt"
                    echo "  ✅ All $test_case_count test cases passed for Python $python_version"
                  else
                    # Some or all test cases for this Python version failed - use temp file to avoid subshell
                    echo "$test_cases" | jq -r '.[]' > "/tmp/current_test_cases.txt"
                    while IFS= read -r test_case; do
                      if [ -n "$test_case" ]; then
                        echo "$test_case (Python $python_version)" >> /tmp/test_results/failed.txt
                        echo "    ❌ Added to failed: $test_case (Python $python_version)"
                      fi
                    done < "/tmp/current_test_cases.txt"
                    echo "  ❌ Some/all $test_case_count test cases failed for Python $python_version"
                  fi
                else
                  echo "No jobs found for Python $python_version"
                  # If no jobs found, mark all test cases as failed for this Python version
                  echo "$test_cases" | jq -r '.[]' > "/tmp/current_test_cases.txt"
                  while IFS= read -r test_case; do
                    if [ -n "$test_case" ]; then
                      echo "$test_case (Python $python_version)" >> /tmp/test_results/failed.txt
                      echo "    ❌ Added to failed (no job found): $test_case (Python $python_version)"
                    fi
                  done < "/tmp/current_test_cases.txt"
                fi
              done
            fi
          fi

          if [ "$matrix_jobs_found" = true ]; then
            # Skip the detailed individual job matching since we have the matrix results
            echo "Matrix job analysis complete. Skipping individual job matching."

            # Debug: Show what was written to result files
            echo "=== DEBUG: Final result files content ==="
            echo "Passed tests (first 10 lines):"
            head -10 /tmp/test_results/passed.txt 2>/dev/null || echo "No passed tests file"
            echo "Failed tests (first 10 lines):"
            head -10 /tmp/test_results/failed.txt 2>/dev/null || echo "No failed tests file"
            echo "=== END DEBUG ==="
          else
            echo "No test matrix jobs found, falling back to individual job matching..."

            # Process each test case across all Python versions
            echo "$test_cases" | jq -r '.[]' | while IFS= read -r test_case; do
              if [ -n "$test_case" ]; then
                echo "Processing test case: $test_case"

                for python_version in "${python_versions[@]}"; do
                  # Try multiple job name patterns since reusable workflows can have different naming
                  job_name_patterns=(
                    "${test_case} (Python ${python_version})"
                    "test-all-cases / ${test_case} (Python ${python_version})"
                    "${test_case}"
                  )

                  job_found=false

                  for job_name in "${job_name_patterns[@]}"; do
                    echo "  Looking for job: $job_name"

                    job_info=$(echo "$jobs_response" | jq -r --arg name "$job_name" '.jobs[] | select(.name == $name) | {id: .id, conclusion: .conclusion}')

                    if [ "$job_info" != "null" ] && [ -n "$job_info" ]; then
                      job_id=$(echo "$job_info" | jq -r '.id')
                      job_conclusion=$(echo "$job_info" | jq -r '.conclusion')

                      echo "    Found job ID: $job_id, conclusion: $job_conclusion"
                      job_found=true

                      if [ "$job_id" != "null" ] && [ -n "$job_id" ]; then
                      # Get job logs for detailed analysis
                      job_logs=$(curl -s -L -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
                        "https://api.github.com/repos/${{ github.repository }}/actions/jobs/$job_id/logs" 2>/dev/null || echo "")
                      if [ -n "$job_logs" ]; then
                        # Check for pass/fail in logs
                        if echo "$job_logs" | grep -q "✅ PASSED: $test_case (Python $python_version)"; then
                          echo "$test_case (Python $python_version)" >> /tmp/test_results/passed.txt
                          echo "    ✅ Test passed based on logs"
                        elif echo "$job_logs" | grep -q "❌ FAILED: $test_case (Python $python_version)"; then
                          echo "$test_case (Python $python_version)" >> /tmp/test_results/failed.txt
                          echo "    ❌ Test failed based on logs"
                        else
                          # Fallback to job conclusion if no specific log markers found
                          if [ "$job_conclusion" = "success" ]; then
                            echo "$test_case (Python $python_version)" >> /tmp/test_results/passed.txt
                            echo "    ✅ Test passed based on job conclusion (no log marker found)"
                          else
                            echo "$test_case (Python $python_version)" >> /tmp/test_results/failed.txt
                            echo "    ❌ Test failed based on job conclusion (no log marker found)"
                          fi
                        fi
                      else
                        # Fallback to job conclusion if logs aren't available
                        if [ "$job_conclusion" = "success" ]; then
                          echo "$test_case (Python $python_version)" >> /tmp/test_results/passed.txt
                          echo "    ✅ Test passed based on job conclusion"
                        else
                          echo "$test_case (Python $python_version)" >> /tmp/test_results/failed.txt
                          echo "    ❌ Test failed based on job conclusion"
                        fi
                      fi
                      fi
                      break # Exit the job_name_patterns loop since we found a match
                    else
                      echo "    Job found but no valid ID: $job_name"
                    fi
                  done

                  if [ "$job_found" = false ]; then
                    echo "    Job not found with any naming pattern for: $test_case (Python $python_version)"
                    # Try to find any job that contains both test_case and python_version in the name
                    matching_job=$(echo "$jobs_response" | jq -r --arg test "$test_case" --arg version "$python_version" \
                      '.jobs[] | select(.name | (contains($test) and contains($version))) | {id: .id, name: .name, conclusion: .conclusion}' | head -1)

                    if [ "$matching_job" != "" ] && [ "$matching_job" != "null" ]; then
                      echo "    Found partial match: $matching_job"
                      job_conclusion=$(echo "$matching_job" | jq -r '.conclusion')
                      if [ "$job_conclusion" = "success" ]; then
                        echo "$test_case (Python $python_version)" >> /tmp/test_results/passed.txt
                        echo "    ✅ Test passed based on partial match job conclusion"
                      else
                        echo "$test_case (Python $python_version)" >> /tmp/test_results/failed.txt
                        echo "    ❌ Test failed based on partial match job conclusion"
                      fi
                    else
                      echo "    No matching job found, assuming failed"
                      echo "$test_case (Python $python_version)" >> /tmp/test_results/failed.txt
                    fi
                  fi
                done
              fi
            done
          fi          # Count results from files - ensure we count each line exactly once
          passed_test_cases=$(cat /tmp/test_results/passed.txt 2>/dev/null | wc -l || echo "0")
          failed_test_cases=$(cat /tmp/test_results/failed.txt 2>/dev/null | wc -l || echo "0")
          total_test_cases=$((passed_test_cases + failed_test_cases))

          echo "Test results summary:"
          echo "  Total: $total_test_cases"
          echo "  Passed: $passed_test_cases"
          echo "  Failed: $failed_test_cases"

          # Validate expected counts
          expected_test_case_count=$(echo "$test_cases" | jq '. | length')
          expected_total=$(( expected_test_case_count * ${#python_versions[@]} ))
          echo "Expected total test cases: $expected_total (${expected_test_case_count} test cases × ${#python_versions[@]} Python versions)"

          if [ "$total_test_cases" -ne "$expected_total" ]; then
            echo "⚠️  WARNING: Actual total ($total_test_cases) doesn't match expected total ($expected_total)"
            echo "This suggests counting logic issues or missing jobs."
          else
            echo "✅ Total test case count matches expected value"
          fi

          # Calculate per-Python version statistics
          mkdir -p /tmp/version_stats
          for python_version in "${python_versions[@]}"; do
            echo "Processing statistics for Python $python_version..."

            passed_count=$(grep -c "Python ${python_version}" /tmp/test_results/passed.txt 2>/dev/null || echo "0")
            failed_count=$(grep -c "Python ${python_version}" /tmp/test_results/failed.txt 2>/dev/null || echo "0")

            # Ensure variables are numeric
            passed_count=$(echo "$passed_count" | tr -d ' \t\n\r')
            failed_count=$(echo "$failed_count" | tr -d ' \t\n\r')
            passed_count="${passed_count:-0}"
            failed_count="${failed_count:-0}"

            total_count=$((passed_count + failed_count))

            # Calculate success rate for this version
            version_success_rate=0
            if [ $total_count -gt 0 ]; then
              version_success_rate=$(( (passed_count * 100) / total_count ))
            fi

            # Ensure proper formatting (prevent "00" display)
            if [ "$failed_count" = "00" ]; then
              failed_count="0"
            fi
            if [ "$passed_count" = "00" ]; then
              passed_count="0"
            fi

            echo "$total_count,$passed_count,$failed_count,$version_success_rate" > "/tmp/version_stats/python_${python_version}.txt"
            echo "Python $python_version FINAL: Total=$total_count, Passed=$passed_count, Failed=$failed_count, Success=$version_success_rate%"
          done

          # Generate email subject
          if [ $failed_test_cases -eq 0 ]; then
            subject="[APICENTER] Weekly APIC Extension Live Test - All Tests Passed ✅"
          else
            subject="[APICENTER] Weekly APIC Extension Live Test - $failed_test_cases/$total_test_cases Tests Failed ❌"
          fi

          # Calculate overall success rate
          success_rate=0
          if [ $total_test_cases -gt 0 ]; then
            success_rate=$(( (passed_test_cases * 100) / total_test_cases ))
          fi

          # Generate HTML email body with Python version breakdown
          cat > /tmp/email_body.html << 'EOF'
          <html>
          <body>
            <h2>APIC Extension Live Test Report</h2>
            <h3>📊 Overall Summary:</h3>
            <table border="1" cellpadding="5" cellspacing="0" style="border-collapse: collapse;">
              <tr>
                <th>Metric</th>
                <th>Total</th>
                <th>Python 3.9</th>
                <th>Python 3.10</th>
                <th>Python 3.11</th>
                <th>Python 3.12</th>
              </tr>
              <tr>
                <td><strong>Total Test Cases</strong></td>
                <td>TOTAL_PLACEHOLDER</td>
                <td>TOTAL_39_PLACEHOLDER</td>
                <td>TOTAL_310_PLACEHOLDER</td>
                <td>TOTAL_311_PLACEHOLDER</td>
                <td>TOTAL_312_PLACEHOLDER</td>
              </tr>
              <tr>
                <td><strong>Passed</strong></td>
                <td>PASSED_PLACEHOLDER</td>
                <td>PASSED_39_PLACEHOLDER</td>
                <td>PASSED_310_PLACEHOLDER</td>
                <td>PASSED_311_PLACEHOLDER</td>
                <td>PASSED_312_PLACEHOLDER</td>
              </tr>
              <tr>
                <td><strong>Failed</strong></td>
                <td>FAILED_PLACEHOLDER</td>
                <td>FAILED_39_PLACEHOLDER</td>
                <td>FAILED_310_PLACEHOLDER</td>
                <td>FAILED_311_PLACEHOLDER</td>
                <td>FAILED_312_PLACEHOLDER</td>
              </tr>
              <tr>
                <td><strong>Success Rate</strong></td>
                <td>SUCCESS_RATE_PLACEHOLDER%</td>
                <td>SUCCESS_RATE_39_PLACEHOLDER%</td>
                <td>SUCCESS_RATE_310_PLACEHOLDER%</td>
                <td>SUCCESS_RATE_311_PLACEHOLDER%</td>
                <td>SUCCESS_RATE_312_PLACEHOLDER%</td>
              </tr>
            </table>
          EOF

          # Replace overall placeholders
          sed -i "s/TOTAL_PLACEHOLDER/$total_test_cases/g" /tmp/email_body.html
          sed -i "s/PASSED_PLACEHOLDER/$passed_test_cases/g" /tmp/email_body.html
          sed -i "s/FAILED_PLACEHOLDER/$failed_test_cases/g" /tmp/email_body.html
          sed -i "s/SUCCESS_RATE_PLACEHOLDER/$success_rate/g" /tmp/email_body.html

          # Replace per-version placeholders
          for python_version in "${python_versions[@]}"; do
            # Convert version to placeholder format (3.9 -> 39, 3.10 -> 310, etc.)
            version_placeholder=$(echo "$python_version" | sed 's/\.//g')

            if [ -f "/tmp/version_stats/python_${python_version}.txt" ]; then
              IFS=',' read -r total_count passed_count failed_count version_success_rate < "/tmp/version_stats/python_${python_version}.txt"

              # Ensure proper formatting of zero values
              if [ "$failed_count" = "00" ]; then
                failed_count="0"
              fi
              if [ "$passed_count" = "00" ]; then
                passed_count="0"
              fi
            else
              total_count=0
              passed_count=0
              failed_count=0
              version_success_rate=0
            fi

            sed -i "s/TOTAL_${version_placeholder}_PLACEHOLDER/$total_count/g" /tmp/email_body.html
            sed -i "s/PASSED_${version_placeholder}_PLACEHOLDER/$passed_count/g" /tmp/email_body.html
            sed -i "s/FAILED_${version_placeholder}_PLACEHOLDER/$failed_count/g" /tmp/email_body.html
            sed -i "s/SUCCESS_RATE_${version_placeholder}_PLACEHOLDER/$version_success_rate/g" /tmp/email_body.html
          done

          # Add failed tests section if any
          if [ $failed_test_cases -gt 0 ] && [ -s /tmp/test_results/failed.txt ]; then
            echo "    <h3>❌ Failed Tests:</h3>" >> /tmp/email_body.html
            echo "    <ul>" >> /tmp/email_body.html
            while IFS= read -r test; do
              if [ -n "$test" ]; then
                echo "      <li>${test}</li>" >> /tmp/email_body.html
              fi
            done < /tmp/test_results/failed.txt
            echo "    </ul>" >> /tmp/email_body.html
          fi

          # Add workflow details
          cat >> /tmp/email_body.html << EOF
            <h3>🔗 Workflow Details:</h3>
            <ul>
              <li><strong>Workflow URL:</strong> <a href="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}">View Workflow Run</a></li>
              <li><strong>Repository:</strong> ${{ github.repository }}</li>
              <li><strong>Branch:</strong> ${{ github.ref_name }}</li>
              <li><strong>Triggered by:</strong> ${{ github.event_name }}</li>
              <li><strong>Run ID:</strong> ${{ github.run_id }}</li>
              <li><strong>Run Attempt:</strong> ${{ github.run_attempt }}</li>
            </ul>
          </body>
          </html>
          EOF

          # Set environment variables for notification step
          {
            echo "EMAIL_TO=frankqian@microsoft.com"
            echo "EMAIL_SUBJECT<<EOF"
            echo "$subject"
            echo "EOF"
            echo "EMAIL_BODY<<EOF"
            cat /tmp/email_body.html
            echo "EOF"
          } >> $GITHUB_ENV

          # Clean up temporary files
          rm -rf /tmp/test_results /tmp/version_stats /tmp/email_body.html /tmp/current_test_cases.txt 2>/dev/null || true

      - name: Send Email Notification
        run: |
          echo "Sending email notification..."

          # Get access token
          response=$(curl -s \
            --request POST \
            --header "Content-Type: application/x-www-form-urlencoded" \
            --data "grant_type=client_credentials&client_id=${{ secrets.MAIL_CLIENT_ID }}&client_secret=${{ secrets.MAIL_CLIENT_SECRET }}&resource=https://management.core.windows.net" \
            "https://login.microsoftonline.com/${{ secrets.MAIL_TENANT_ID }}/oauth2/token")

          access_token=$(echo $response | jq -r '.access_token // empty')

          if [ -z "$access_token" ]; then
            echo "Failed to get access token"
            echo "Response: $response"
            exit 1
          fi

          # Create JSON payload using jq to properly escape special characters
          json_payload=$(jq -n \
            --arg to "$EMAIL_TO" \
            --arg subject "$EMAIL_SUBJECT" \
            --arg body "$EMAIL_BODY" \
            '{to: $to, subject: $subject, body: $body}')

          # Send email
          curl_response=$(curl -s -w "HTTP_STATUS:%{http_code}" \
            --request POST \
            --header "Content-Type: application/json" \
            --header "Authorization: Bearer $access_token" \
            --data "$json_payload" \
            'https://prod-18.northcentralus.logic.azure.com:443/workflows/b33d7861bfc64832a6f62cc8f2213988/triggers/manual/paths/invoke?api-version=2016-10-01&sp=%2Ftriggers%2Fmanual%2Frun&sv=1.0')

          http_status=$(echo "$curl_response" | grep -o "HTTP_STATUS:[0-9]*" | cut -d: -f2)
          response_body=$(echo "$curl_response" | sed 's/HTTP_STATUS:[0-9]*$//')

          echo "Email sending status: $http_status"
          if [ "$http_status" != "200" ] && [ "$http_status" != "202" ]; then
            echo "Failed to send email. Response: $response_body"
            exit 1
          else
            echo "Email sent successfully"
          fi
